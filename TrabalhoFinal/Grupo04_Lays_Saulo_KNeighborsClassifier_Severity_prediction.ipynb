{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Grupo04_Lays_Saulo_KNeighborsClassifier_Severity_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZA93r5ZWkv"
      },
      "source": [
        "# Classificação de Texto\n",
        "\n",
        "Exemplo de classificação de texto com pipeline de NLP e classificador no final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx_2ZKqmZWk3"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JRf9OFjhV0x",
        "outputId": "54ed3b4e-5d6e-49b2-e103-f3cca4cc4517"
      },
      "source": [
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sbOi1hdZWk7"
      },
      "source": [
        "# Tokenização\n",
        "def tokenize(text):\n",
        "    # Tokenização\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Stemização\n",
        "    stems  = []\n",
        "    for item in tokens:\n",
        "        stems.append(SnowballStemmer(\"portuguese\").stem(item))\n",
        "    return stems"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zLolLTX0e_U",
        "outputId": "f797d22e-b6dd-41a0-db04-a6b01bd7d62b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOQfc4VbH8q_"
      },
      "source": [
        "def ProcessedData(df):\n",
        "\n",
        "  # Método para ordenar os bugs em ordem cronológica de acordo com o tempo de criação\n",
        "  df = df.sort_values(by=['Bug_report_ID'])\n",
        "\n",
        "  # Removendo entradas diferentes de \"blocker\", \"critical\", \"major\",\"minor\" e \"trivial\"\n",
        "  processedDataset = df[(df.severity_level == \"blocker\") | (df.severity_level == \"critical\") | (df.severity_level == \"major\") | (df.severity_level == \"minor\") | (df.severity_level == \"trivial\")]\n",
        "\n",
        "  #Concatenando colunas\n",
        "  processedDataset[\"combinedSumAndDesc\"] = processedDataset[\"summary\"] + processedDataset[\"description\"]\n",
        "\n",
        "  #Removendo colunas desnecessárias\n",
        "  processedDataset.drop('summary', inplace=True, axis=1)\n",
        "  processedDataset.drop('description', inplace=True, axis=1)\n",
        "  processedDataset.drop('component_name', inplace=True, axis=1)\n",
        "\n",
        "  #Retirando valores NaN\n",
        "  processedDataset['combinedSumAndDesc'].isnull().sum()\n",
        "  processedDataset = processedDataset.dropna()\n",
        "\n",
        "  return processedDataset"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu65vpaoJoHl"
      },
      "source": [
        "# Realiza a separação do dataframe\n",
        "def SplitDataSet(dataset):\n",
        "  # Calcula tamanho\n",
        "  dataSetLen = len(dataset) // 11\n",
        "\n",
        "  # Calcula indexes\n",
        "  firstIdx = 0\n",
        "  lastIdx = 0\n",
        "\n",
        "  dfs = []\n",
        "  for i in range(11):\n",
        "    # Calculando os indexes para retirar os dados do dataframe\n",
        "    if(i != 0):\n",
        "      firstIdx += dataSetLen\n",
        "    lastIdx += dataSetLen\n",
        "\n",
        "    # Realizando o iloc do datraframe entre firstIdx e lastIdx\n",
        "    dfs.append(dataset.iloc[firstIdx:lastIdx])\n",
        "\n",
        "  return dfs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxi8SRW-INJj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrHj_VyjZWlC"
      },
      "source": [
        "# Realiza o treinamento e retorna a acurácia\n",
        "def TrainDataset(X_train,X_test,y_train,y_test):\n",
        "  # Stop Words\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  # NLP Pipeline\n",
        "  text_clf   = Pipeline([\n",
        "                  # Vectorize\n",
        "                  ('vect',  TfidfVectorizer(tokenizer=tokenize, \n",
        "                                            stop_words=stop_words, \n",
        "                                            ngram_range=(1,1))),\n",
        "                  # Classificador\n",
        "                  ('clf',   KNeighborsClassifier(n_jobs=-1)),\n",
        "              ])\n",
        "\n",
        "  # Train\n",
        "  text_clf = text_clf.fit(X_train.combinedSumAndDesc, y_train)\n",
        "  # Test \n",
        "  predictions = text_clf.predict(X_test.combinedSumAndDesc)\n",
        "  f1 = f1_score(y_test, predictions, average='micro')\n",
        "\n",
        "  return f1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "ML7Oy3I4hpls",
        "outputId": "7ab803c0-c3e8-4228-81cd-e351f6944914"
      },
      "source": [
        "for datasetNum in range(5):\n",
        "  print(f\"===>Training dataset: {datasetNum}\")\n",
        "\n",
        "  # Carregando o dataset\n",
        "  if datasetNum == 0:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Eclipse_total.csv')\n",
        "  elif datasetNum == 1:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/GCC_total.csv')\n",
        "  elif datasetNum == 2:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Mozilla_total.csv')\n",
        "  elif datasetNum == 3:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Netbeans_total.csv')\n",
        "  elif datasetNum == 4:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/OpenOffice_total.csv')\n",
        "\n",
        "  processedDataFrame = ProcessedData(loadedDataset)\n",
        "\n",
        "  dfs = SplitDataSet(processedDataFrame)\n",
        "\n",
        "  all_accuracy = []\n",
        "\n",
        "  for i in range(10):\n",
        "    combinedDataFrame = dfs[i].append(dfs[i+1])\n",
        "    # Split Dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(combinedDataFrame[['combinedSumAndDesc']], combinedDataFrame.severity_level, random_state=42)\n",
        "\n",
        "    # Treinando os dados e pegando a acurácia\n",
        "    Trained = TrainDataset(X_train, X_test, y_train, y_test)\n",
        "    all_accuracy.append(Trained)\n",
        "\n",
        "    # Atualizando o dataframe para pegarmos o combinado na próxima iteração\n",
        "    dfs[i+1] = combinedDataFrame\n",
        "\n",
        "    # Calculando a acurácia final\n",
        "    finalAcc = 0\n",
        "\n",
        "    # Somando as acurácias\n",
        "    for j in range(10):\n",
        "      print(f\"\\tDataset {datasetNum}: Accuracy from training {j}: {all_accuracy[j]}\")\n",
        "      finalAcc += all_accuracy[j]\n",
        "\n",
        "    # Mostrando a acurácia média\n",
        "    print(f\"\\t\\tDataset {datasetNum} final accuracy: {finalAcc / 10}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===>Training dataset: 0\n",
            "\tDataset 0: Accuracy from training 0: 0.41194029850746267\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8a41f59c79b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Somando as acurácias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tDataset {datasetNum}: Accuracy from training {j}: {all_accuracy[j]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mfinalAcc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mall_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}