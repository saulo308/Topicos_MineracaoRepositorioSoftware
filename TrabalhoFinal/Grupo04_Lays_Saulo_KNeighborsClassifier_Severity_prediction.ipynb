{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Grupo04_Lays_Saulo_KNeighborsClassifier_Severity_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZA93r5ZWkv"
      },
      "source": [
        "# Classificação de Texto\n",
        "\n",
        "Exemplo de classificação de texto com pipeline de NLP e classificador no final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx_2ZKqmZWk3"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JRf9OFjhV0x",
        "outputId": "54ed3b4e-5d6e-49b2-e103-f3cca4cc4517"
      },
      "source": [
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sbOi1hdZWk7"
      },
      "source": [
        "# Tokenização\n",
        "def tokenize(text):\n",
        "    # Tokenização\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Stemização\n",
        "    stems  = []\n",
        "    for item in tokens:\n",
        "        stems.append(SnowballStemmer(\"portuguese\").stem(item))\n",
        "    return stems"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zLolLTX0e_U",
        "outputId": "f797d22e-b6dd-41a0-db04-a6b01bd7d62b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOQfc4VbH8q_"
      },
      "source": [
        "def ProcessedData(df):\n",
        "\n",
        "  # Método para ordenar os bugs em ordem cronológica de acordo com o tempo de criação\n",
        "  df = df.sort_values(by=['Bug_report_ID'])\n",
        "\n",
        "  # Removendo entradas diferentes de \"blocker\", \"critical\", \"major\",\"minor\" e \"trivial\"\n",
        "  processedDataset = df[(df.severity_level == \"blocker\") | (df.severity_level == \"critical\") | (df.severity_level == \"major\") | (df.severity_level == \"minor\") | (df.severity_level == \"trivial\")]\n",
        "\n",
        "  #Concatenando colunas\n",
        "  processedDataset[\"combinedSumAndDesc\"] = processedDataset[\"summary\"] + processedDataset[\"description\"]\n",
        "\n",
        "  #Removendo colunas desnecessárias\n",
        "  processedDataset.drop('summary', inplace=True, axis=1)\n",
        "  processedDataset.drop('description', inplace=True, axis=1)\n",
        "  processedDataset.drop('component_name', inplace=True, axis=1)\n",
        "\n",
        "  #Retirando valores NaN\n",
        "  processedDataset['combinedSumAndDesc'].isnull().sum()\n",
        "  processedDataset = processedDataset.dropna()\n",
        "\n",
        "  return processedDataset"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu65vpaoJoHl"
      },
      "source": [
        "# Realiza a separação do dataframe\n",
        "def SplitDataSet(dataset):\n",
        "  # Calcula tamanho\n",
        "  dataSetLen = len(dataset) // 11\n",
        "\n",
        "  # Calcula indexes\n",
        "  firstIdx = 0\n",
        "  lastIdx = 0\n",
        "\n",
        "  dfs = []\n",
        "  for i in range(11):\n",
        "    # Calculando os indexes para retirar os dados do dataframe\n",
        "    if(i != 0):\n",
        "      firstIdx += dataSetLen\n",
        "    lastIdx += dataSetLen\n",
        "\n",
        "    # Realizando o iloc do datraframe entre firstIdx e lastIdx\n",
        "    dfs.append(dataset.iloc[firstIdx:lastIdx])\n",
        "\n",
        "  return dfs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxi8SRW-INJj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrHj_VyjZWlC"
      },
      "source": [
        "# Realiza o treinamento e retorna a acurácia\n",
        "def TrainDataset(X_train,X_test,y_train,y_test):\n",
        "  # Stop Words\n",
        "  stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "  # NLP Pipeline\n",
        "  text_clf   = Pipeline([\n",
        "                  # Vectorize\n",
        "                  ('vect',  TfidfVectorizer(tokenizer=tokenize, \n",
        "                                            stop_words=stop_words, \n",
        "                                            ngram_range=(1,1))),\n",
        "                  # Classificador\n",
        "                  ('clf',   KNeighborsClassifier(n_jobs=-1)),\n",
        "              ])\n",
        "\n",
        "  # Train\n",
        "  text_clf = text_clf.fit(X_train.combinedSumAndDesc, y_train)\n",
        "  # Test \n",
        "  predictions = text_clf.predict(X_test.combinedSumAndDesc)\n",
        "  f1 = f1_score(y_test, predictions, average='micro')\n",
        "\n",
        "  return f1"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML7Oy3I4hpls",
        "outputId": "48177b60-c193-4971-9385-23d58c3d87e2"
      },
      "source": [
        "for datasetNum in range(5):\n",
        "  print(f\"===>Training dataset: {datasetNum}\")\n",
        "\n",
        "  # Carregando o dataset\n",
        "  if datasetNum == 0:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Eclipse_total.csv')\n",
        "  elif datasetNum == 1:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/GCC_total.csv')\n",
        "  elif datasetNum == 2:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Mozilla_total.csv')\n",
        "  elif datasetNum == 3:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/Netbeans_total.csv')\n",
        "  elif datasetNum == 4:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Trabalho_Final/Case_Study/OpenOffice_total.csv')\n",
        "\n",
        "  processedDataFrame = ProcessedData(loadedDataset)\n",
        "\n",
        "  dfs = SplitDataSet(processedDataFrame)\n",
        "\n",
        "  all_accuracy = []\n",
        "\n",
        "  for i in range(10):\n",
        "    combinedDataFrame = dfs[i].append(dfs[i+1])\n",
        "    # Split Dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(combinedDataFrame[['combinedSumAndDesc']], combinedDataFrame.severity_level, random_state=42)\n",
        "\n",
        "    # Treinando os dados e pegando a acurácia\n",
        "    Trained = TrainDataset(X_train, X_test, y_train, y_test)\n",
        "    all_accuracy.append(Trained)\n",
        "\n",
        "    # Atualizando o dataframe para pegarmos o combinado na próxima iteração\n",
        "    dfs[i+1] = combinedDataFrame\n",
        "\n",
        "    # Calculando a acurácia final\n",
        "    finalAcc = 0\n",
        "\n",
        "  # Somando as acurácias\n",
        "  for j in range(10):\n",
        "    print(f\"\\tDataset {datasetNum}: Accuracy from training {j}: {all_accuracy[j]}\")\n",
        "    finalAcc += all_accuracy[j]\n",
        "\n",
        "  # Mostrando a acurácia média\n",
        "  print(f\"\\t\\tDataset {datasetNum} final accuracy: {finalAcc / 10}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===>Training dataset: 0\n",
            "\tDataset 0: Accuracy from training 0: 0.4000000000000001\n",
            "\tDataset 0: Accuracy from training 1: 0.40159045725646125\n",
            "\tDataset 0: Accuracy from training 2: 0.4343283582089552\n",
            "\tDataset 0: Accuracy from training 3: 0.41408114558472553\n",
            "\tDataset 0: Accuracy from training 4: 0.40696517412935324\n",
            "\tDataset 0: Accuracy from training 5: 0.4441602728047741\n",
            "\tDataset 0: Accuracy from training 6: 0.43656716417910446\n",
            "\tDataset 0: Accuracy from training 7: 0.4204244031830239\n",
            "\tDataset 0: Accuracy from training 8: 0.41074626865671643\n",
            "\tDataset 0: Accuracy from training 9: 0.41942485078676073\n",
            "\t\tDataset 0 final accuracy: 0.4188288094789875\n",
            "===>Training dataset: 1\n",
            "\tDataset 1: Accuracy from training 0: 0.992\n",
            "\tDataset 1: Accuracy from training 1: 0.9893048128342246\n",
            "\tDataset 1: Accuracy from training 2: 0.9678714859437751\n",
            "\tDataset 1: Accuracy from training 3: 0.9198717948717948\n",
            "\tDataset 1: Accuracy from training 4: 0.8796791443850268\n",
            "\tDataset 1: Accuracy from training 5: 0.8555045871559633\n",
            "\tDataset 1: Accuracy from training 6: 0.8012048192771084\n",
            "\tDataset 1: Accuracy from training 7: 0.7415329768270945\n",
            "\tDataset 1: Accuracy from training 8: 0.6934189406099518\n",
            "\tDataset 1: Accuracy from training 9: 0.6525547445255474\n",
            "\t\tDataset 1 final accuracy: 0.8492943306430487\n",
            "===>Training dataset: 2\n",
            "\tDataset 2: Accuracy from training 0: 0.31666666666666665\n",
            "\tDataset 2: Accuracy from training 1: 0.45555555555555555\n",
            "\tDataset 2: Accuracy from training 2: 0.4351464435146444\n",
            "\tDataset 2: Accuracy from training 3: 0.37458193979933113\n",
            "\tDataset 2: Accuracy from training 4: 0.4178272980501393\n",
            "\tDataset 2: Accuracy from training 5: 0.43914081145584727\n",
            "\tDataset 2: Accuracy from training 6: 0.4372384937238494\n",
            "\tDataset 2: Accuracy from training 7: 0.4312267657992565\n",
            "\tDataset 2: Accuracy from training 8: 0.44481605351170567\n",
            "\tDataset 2: Accuracy from training 9: 0.4848024316109423\n",
            "\t\tDataset 2 final accuracy: 0.4237002459687938\n",
            "===>Training dataset: 3\n",
            "\tDataset 3: Accuracy from training 0: 1.0\n",
            "\tDataset 3: Accuracy from training 1: 1.0\n",
            "\tDataset 3: Accuracy from training 2: 1.0\n",
            "\tDataset 3: Accuracy from training 3: 1.0\n",
            "\tDataset 3: Accuracy from training 4: 1.0\n",
            "\tDataset 3: Accuracy from training 5: 0.9986376021798365\n",
            "\tDataset 3: Accuracy from training 6: 0.9976152623211447\n",
            "\tDataset 3: Accuracy from training 7: 0.9982338396326387\n",
            "\tDataset 3: Accuracy from training 8: 0.9930047694753578\n",
            "\tDataset 3: Accuracy from training 9: 0.9901734104046243\n",
            "\t\tDataset 3 final accuracy: 0.9977664884013603\n",
            "===>Training dataset: 4\n",
            "\tDataset 4: Accuracy from training 0: 1.0\n",
            "\tDataset 4: Accuracy from training 1: 0.9993451211525868\n",
            "\tDataset 4: Accuracy from training 2: 0.99950884086444\n",
            "\tDataset 4: Accuracy from training 3: 0.999607072691552\n",
            "\tDataset 4: Accuracy from training 4: 0.9993451211525868\n",
            "\tDataset 4: Accuracy from training 5: 0.9994386752736458\n",
            "\tDataset 4: Accuracy from training 6: 0.9987721021611002\n",
            "\tDataset 4: Accuracy from training 7: 0.9993451211525868\n",
            "\tDataset 4: Accuracy from training 8: 0.9978388998035363\n",
            "\tDataset 4: Accuracy from training 9: 0.9639221289515985\n",
            "\t\tDataset 4 final accuracy: 0.9957123083203632\n"
          ]
        }
      ]
    }
  ]
}