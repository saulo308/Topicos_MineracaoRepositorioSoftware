{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"Grupo04_Lays_Saulo_MultinomialNB_Severity_prediction.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"YHZA93r5ZWkv"},"source":["# Classificação de Texto no contexto da Predição de Severidade de Bug Reports\n","\n","Exemplo de classificação de texto com pipeline de NLP e classificador no final"]},{"cell_type":"code","metadata":{"id":"Nx_2ZKqmZWk3","executionInfo":{"status":"ok","timestamp":1633480197913,"user_tz":180,"elapsed":1898,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}}},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.snowball import SnowballStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","#Classificadores\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sbOi1hdZWk7","executionInfo":{"status":"ok","timestamp":1633480197913,"user_tz":180,"elapsed":5,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}}},"source":["# Tokenização\n","def tokenize(text):\n","    # Tokenização\n","    tokens = nltk.word_tokenize(text)\n","    \n","    # Stemização\n","    stems  = []\n","    for item in tokens:\n","        stems.append(SnowballStemmer(\"english\").stem(item))\n","    return stems"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zLolLTX0e_U","executionInfo":{"status":"ok","timestamp":1633480215480,"user_tz":180,"elapsed":16365,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}},"outputId":"b68b6f6b-59c9-48a8-c580-00c9e0447bb3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"KpnF6ng94Jzw","executionInfo":{"status":"ok","timestamp":1633480217536,"user_tz":180,"elapsed":239,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}}},"source":["#Realiza as modificações no dataset\n","def GetProcessedDataset(df):\n","  #Convertendo 'summary' e 'description' para string\n","  df = df.astype({'summary':'str', 'description':'str'})\n","\n","  #Filtrando severidades confiaveis\n","  df = df.loc[(df['severity_level'] != 'enhancement') & (df['severity_level'] != 'normal')]\n","\n","  #Modificando severidade diferente de blocker (Classificação binaria)\n","  df.loc[(df['severity_level'] != 'blocker'), 'severity_level'] = \"no-blocker\"\n","\n","  return df"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SInJYwwc5cem","executionInfo":{"status":"ok","timestamp":1633480220011,"user_tz":180,"elapsed":871,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}},"outputId":"6d529881-c004-42e4-bff3-8315f7cef185"},"source":["# Baixando as Stop Words\n","nltk.download('stopwords')\n","\n","nltk.download('punkt')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Ma-g9AtNpGSe","executionInfo":{"status":"ok","timestamp":1633480221351,"user_tz":180,"elapsed":233,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}}},"source":["# Realiza a separação do dataframe\n","def SplitDataSet(dataset):\n","  # Calcula tamanho\n","  dataSetLen = len(dataset) // 11\n","\n","  # Calcula indexes\n","  firstIdx = 0\n","  lastIdx = 0\n","\n","  dfs = []\n","  for i in range(11):\n","    # Calculando os indexes para retirar os dados do dataframe\n","    if(i != 0):\n","      firstIdx += dataSetLen\n","    lastIdx += dataSetLen\n","\n","    # Realizando o iloc do datraframe entre firstIdx e lastIdx\n","    dfs.append(dataset.iloc[firstIdx:lastIdx])\n","\n","  return dfs"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vDZdCVgvJLD","executionInfo":{"status":"ok","timestamp":1633480222994,"user_tz":180,"elapsed":230,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}}},"source":["# Realiza o treinamento e retorna a acurácia\n","def TrainDataset(X_train,X_test,y_train,y_test):\n","  stop_words = nltk.corpus.stopwords.words('english')\n","\n","  # NLP Pipeline\n","  text_clf   = Pipeline([\n","                  # Vectorize\n","                  ('vect',  TfidfVectorizer(tokenizer=tokenize, \n","                                            stop_words=stop_words, \n","                                            ngram_range=(1,2))),\n","                  # Classificador\n","                  ('clf',   MultinomialNB()),\n","              ])\n","\n","  # Train\n","  text_clf = text_clf.fit(X_train.description, y_train)\n","\n","  # Testando e retornando a acurácia\n","  pred = text_clf.predict(X_test.description)\n","  accuracy_scoreMLP_TFIDF = f1_score(y_test, pred, average='micro')\n","\n","  return accuracy_scoreMLP_TFIDF"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I2usSv8I5Fjv","executionInfo":{"status":"ok","timestamp":1633480875277,"user_tz":180,"elapsed":649679,"user":{"displayName":"SAULO SOARES DE OLIVEIRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikvHHDQAOVXhUIabb5qb7ZKAEDJBQ2bUiUmJeh=s64","userId":"05647746374611919360"}},"outputId":"170f7e9f-883e-4695-a6b6-c7ee0616f8a7"},"source":["for datasetNum in range(5):\n","  print(f\"===>Training dataset: {datasetNum}\")\n","\n","  # Carregando o dataset\n","  if datasetNum == 0:\n","    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Eclipse_total.csv')\n","  elif datasetNum == 1:\n","    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/GCC_total.csv')\n","  elif datasetNum == 2:\n","    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Mozilla_total.csv')\n","  elif datasetNum == 3:\n","    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Netbeans_total.csv')\n","  elif datasetNum == 4:\n","    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/OpenOffice_total.csv')\n","\n","  # Realizando o processamento do dataset (pré-processamento)\n","  processedDataframe = GetProcessedDataset(loadedDataset)\n","\n","  # Divindo o dataframe em 11 partes\n","  dfs = SplitDataSet(processedDataframe)\n","\n","  # Guarda as acurácias\n","  all_accuracy = []\n","\n","  # Treinando 10 vezes, a partir das combinações\n","  for i in range(10):\n","    # Combinando os dataframes i e i + 1 para teste e treino\n","    combinedDataframe = dfs[i].append(dfs[i+1])\n","\n","    # Realizando o split\n","    X_train, X_test, y_train, y_test = train_test_split(combinedDataframe[['description']], combinedDataframe.severity_level, random_state=42)\n","\n","    # Treinando os dados e pegando a acurácia\n","    trainedAcc = TrainDataset(X_train,X_test,y_train,y_test)\n","    all_accuracy.append(trainedAcc)\n","\n","    # Atualizando o dataframe para pegarmos o combinado na próxima iteração\n","    dfs[i+1] = combinedDataframe\n","\n","  # Calculando a acurácia final\n","  finalAcc = 0\n","\n","  # Somando as acurácias\n","  for j in range(10):\n","    print(f\"\\tDataset {datasetNum}: Accuracy from training {j}: {all_accuracy[j]}\")\n","    finalAcc += all_accuracy[j]\n","\n","  # Mostrando a acurácia média\n","  print(f\"\\t\\tDataset {datasetNum} final accuracy: {finalAcc / 10}\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["===>Training dataset: 0\n","\tDataset 0: Accuracy from training 0: 0.8520710059171598\n","\tDataset 0: Accuracy from training 1: 0.8658777120315583\n","\tDataset 0: Accuracy from training 2: 0.8476331360946747\n","\tDataset 0: Accuracy from training 3: 0.842603550295858\n","\tDataset 0: Accuracy from training 4: 0.8717948717948718\n","\tDataset 0: Accuracy from training 5: 0.871513102282333\n","\tDataset 0: Accuracy from training 6: 0.8846153846153846\n","\tDataset 0: Accuracy from training 7: 0.8915187376725838\n","\tDataset 0: Accuracy from training 8: 0.9029585798816568\n","\tDataset 0: Accuracy from training 9: 0.8977945131791286\n","\t\tDataset 0 final accuracy: 0.8728380593765209\n","===>Training dataset: 1\n","\tDataset 1: Accuracy from training 0: 0.816\n","\tDataset 1: Accuracy from training 1: 0.8663101604278075\n","\tDataset 1: Accuracy from training 2: 0.9236947791164659\n","\tDataset 1: Accuracy from training 3: 0.9102564102564102\n","\tDataset 1: Accuracy from training 4: 0.9197860962566845\n","\tDataset 1: Accuracy from training 5: 0.9243119266055045\n","\tDataset 1: Accuracy from training 6: 0.9317269076305221\n","\tDataset 1: Accuracy from training 7: 0.9233511586452763\n","\tDataset 1: Accuracy from training 8: 0.9261637239165329\n","\tDataset 1: Accuracy from training 9: 0.9284671532846716\n","\t\tDataset 1 final accuracy: 0.9070068316139877\n","===>Training dataset: 2\n","\tDataset 2: Accuracy from training 0: 0.943089430894309\n","\tDataset 2: Accuracy from training 1: 0.9239130434782609\n","\tDataset 2: Accuracy from training 2: 0.963265306122449\n","\tDataset 2: Accuracy from training 3: 0.9153094462540716\n","\tDataset 2: Accuracy from training 4: 0.9293478260869565\n","\tDataset 2: Accuracy from training 5: 0.9324009324009324\n","\tDataset 2: Accuracy from training 6: 0.9244897959183674\n","\tDataset 2: Accuracy from training 7: 0.9365942028985508\n","\tDataset 2: Accuracy from training 8: 0.9265905383360522\n","\tDataset 2: Accuracy from training 9: 0.9495548961424333\n","\t\tDataset 2 final accuracy: 0.9344555418532383\n","===>Training dataset: 3\n","\tDataset 3: Accuracy from training 0: 0.9700149925037481\n","\tDataset 3: Accuracy from training 1: 0.976\n","\tDataset 3: Accuracy from training 2: 0.9752438109527382\n","\tDataset 3: Accuracy from training 3: 0.9844031193761248\n","\tDataset 3: Accuracy from training 4: 0.9855\n","\tDataset 3: Accuracy from training 5: 0.9845692241748821\n","\tDataset 3: Accuracy from training 6: 0.9894973743435859\n","\tDataset 3: Accuracy from training 7: 0.9816666666666667\n","\tDataset 3: Accuracy from training 8: 0.9825982598259826\n","\tDataset 3: Accuracy from training 9: 0.9814511729405346\n","\t\tDataset 3 final accuracy: 0.9810944620784262\n","===>Training dataset: 4\n","\tDataset 4: Accuracy from training 0: 0.9912195121951219\n","\tDataset 4: Accuracy from training 1: 0.9908972691807543\n","\tDataset 4: Accuracy from training 2: 0.9941463414634146\n","\tDataset 4: Accuracy from training 3: 0.9941474834178697\n","\tDataset 4: Accuracy from training 4: 0.9938211382113821\n","\tDataset 4: Accuracy from training 5: 0.9952619843924192\n","\tDataset 4: Accuracy from training 6: 0.9960975609756098\n","\tDataset 4: Accuracy from training 7: 0.9960979839583785\n","\tDataset 4: Accuracy from training 8: 0.9966829268292683\n","\tDataset 4: Accuracy from training 9: 0.9960979070592408\n","\t\tDataset 4 final accuracy: 0.994447010768346\n"]}]}]}