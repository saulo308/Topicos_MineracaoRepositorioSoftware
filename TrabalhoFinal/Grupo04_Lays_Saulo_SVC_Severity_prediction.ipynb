{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Grupo04_Lays_Saulo_SVC_Severity_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHZA93r5ZWkv"
      },
      "source": [
        "# Classificação de Texto no contexto da Predição de Severidade de Bug Reports\n",
        "\n",
        "Exemplo de classificação de texto com pipeline de NLP e classificador no final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx_2ZKqmZWk3"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "#Classificadores\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sbOi1hdZWk7"
      },
      "source": [
        "# Tokenização\n",
        "def tokenize(text):\n",
        "    # Tokenização\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Stemização\n",
        "    stems  = []\n",
        "    for item in tokens:\n",
        "        stems.append(SnowballStemmer(\"english\").stem(item))\n",
        "    return stems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zLolLTX0e_U",
        "outputId": "ede76e8d-94e0-474e-b275-864c30ffbdd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpnF6ng94Jzw"
      },
      "source": [
        "#Realiza as modificações no dataset\n",
        "def GetProcessedDataset(df):\n",
        "  #Convertendo 'summary' e 'description' para string\n",
        "  df = df.astype({'summary':'str', 'description':'str'})\n",
        "\n",
        "  #Filtrando severidades confiaveis\n",
        "  df = df.loc[(df['severity_level'] != 'enhancement') & (df['severity_level'] != 'normal')]\n",
        "\n",
        "  #Modificando severidade diferente de blocker (Classificação binaria)\n",
        "  df.loc[(df['severity_level'] != 'blocker'), 'severity_level'] = \"no-blocker\"\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInJYwwc5cem",
        "outputId": "69f05584-f30b-4a78-a091-9b80f0855ee3"
      },
      "source": [
        "# Baixando as Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma-g9AtNpGSe"
      },
      "source": [
        "# Realiza a separação do dataframe\n",
        "def SplitDataSet(dataset):\n",
        "  # Calcula tamanho\n",
        "  dataSetLen = len(dataset) // 11\n",
        "\n",
        "  # Calcula indexes\n",
        "  firstIdx = 0\n",
        "  lastIdx = 0\n",
        "\n",
        "  dfs = []\n",
        "  for i in range(11):\n",
        "    # Calculando os indexes para retirar os dados do dataframe\n",
        "    if(i != 0):\n",
        "      firstIdx += dataSetLen\n",
        "    lastIdx += dataSetLen\n",
        "\n",
        "    # Realizando o iloc do datraframe entre firstIdx e lastIdx\n",
        "    dfs.append(dataset.iloc[firstIdx:lastIdx])\n",
        "\n",
        "  return dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vDZdCVgvJLD"
      },
      "source": [
        "# Realiza o treinamento e retorna a acurácia\n",
        "def TrainDataset(X_train,X_test,y_train,y_test):\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  # NLP Pipeline\n",
        "  text_clf   = Pipeline([\n",
        "                  # Vectorize\n",
        "                  ('vect',  TfidfVectorizer(tokenizer=tokenize, \n",
        "                                            stop_words=stop_words, \n",
        "                                            ngram_range=(1,2))),\n",
        "                  # Classificador\n",
        "                  ('clf',   SVC(kernel='linear', C=1E10)),\n",
        "              ])\n",
        "\n",
        "  # Train\n",
        "  text_clf = text_clf.fit(X_train.description, y_train)\n",
        "\n",
        "  # Testando e retornando a acurácia\n",
        "  pred = text_clf.predict(X_test.description)\n",
        "  accuracy_scoreMLP_TFIDF = f1_score(y_test, pred, average='micro')\n",
        "\n",
        "  return accuracy_scoreMLP_TFIDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2usSv8I5Fjv",
        "outputId": "10c1f379-cd1f-4f66-c4a4-f6d4e53ea8b6"
      },
      "source": [
        "for datasetNum in range(5):\n",
        "  print(f\"===>Training dataset: {datasetNum}\")\n",
        "\n",
        "  # Carregando o dataset\n",
        "  if datasetNum == 0:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Eclipse_total.csv')\n",
        "  elif datasetNum == 1:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/GCC_total.csv')\n",
        "  elif datasetNum == 2:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Mozilla_total.csv')\n",
        "  elif datasetNum == 3:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/Netbeans_total.csv')\n",
        "  elif datasetNum == 4:\n",
        "    loadedDataset = pd.read_csv('/content/drive/My Drive/TrabFinal_Mineracao/Datasets/OpenOffice_total.csv')\n",
        "\n",
        "  # Realizando o processamento do dataset (pré-processamento)\n",
        "  processedDataframe = GetProcessedDataset(loadedDataset)\n",
        "\n",
        "  # Divindo o dataframe em 11 partes\n",
        "  dfs = SplitDataSet(processedDataframe)\n",
        "\n",
        "  # Guarda as acurácias\n",
        "  all_accuracy = []\n",
        "\n",
        "  # Treinando 10 vezes, a partir das combinações\n",
        "  for i in range(10):\n",
        "    # Combinando os dataframes i e i + 1 para teste e treino\n",
        "    combinedDataframe = dfs[i].append(dfs[i+1])\n",
        "\n",
        "    # Realizando o split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(combinedDataframe[['description']], combinedDataframe.severity_level, random_state=42)\n",
        "\n",
        "    # Treinando os dados e pegando a acurácia\n",
        "    trainedAcc = TrainDataset(X_train,X_test,y_train,y_test)\n",
        "    all_accuracy.append(trainedAcc)\n",
        "\n",
        "    # Atualizando o dataframe para pegarmos o combinado na próxima iteração\n",
        "    dfs[i+1] = combinedDataframe\n",
        "\n",
        "  # Calculando a acurácia final\n",
        "  finalAcc = 0\n",
        "\n",
        "  # Somando as acurácias\n",
        "  for j in range(10):\n",
        "    print(f\"\\tDataset {datasetNum}: Accuracy from training {j}: {all_accuracy[j]}\")\n",
        "    finalAcc += all_accuracy[j]\n",
        "\n",
        "  # Mostrando a acurácia média\n",
        "  print(f\"\\t\\tDataset {datasetNum} final accuracy: {finalAcc / 10}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===>Training dataset: 0\n",
            "\tDataset 0: Accuracy from training 0: 0.8372781065088757\n",
            "\tDataset 0: Accuracy from training 1: 0.8639053254437871\n",
            "\tDataset 0: Accuracy from training 2: 0.8609467455621301\n",
            "\tDataset 0: Accuracy from training 3: 0.834319526627219\n",
            "\tDataset 0: Accuracy from training 4: 0.8648915187376726\n",
            "\tDataset 0: Accuracy from training 5: 0.8605240912933221\n",
            "\tDataset 0: Accuracy from training 6: 0.8809171597633136\n",
            "\tDataset 0: Accuracy from training 7: 0.886259040105194\n",
            "\tDataset 0: Accuracy from training 8: 0.8940828402366863\n",
            "\tDataset 0: Accuracy from training 9: 0.8983324367939752\n",
            "\t\tDataset 0 final accuracy: 0.8681456791072175\n",
            "===>Training dataset: 1\n",
            "\tDataset 1: Accuracy from training 0: 0.864\n",
            "\tDataset 1: Accuracy from training 1: 0.8663101604278075\n",
            "\tDataset 1: Accuracy from training 2: 0.9196787148594378\n",
            "\tDataset 1: Accuracy from training 3: 0.9198717948717948\n",
            "\tDataset 1: Accuracy from training 4: 0.9197860962566845\n",
            "\tDataset 1: Accuracy from training 5: 0.9220183486238532\n",
            "\tDataset 1: Accuracy from training 6: 0.9417670682730923\n",
            "\tDataset 1: Accuracy from training 7: 0.928698752228164\n",
            "\tDataset 1: Accuracy from training 8: 0.9293739967897271\n",
            "\tDataset 1: Accuracy from training 9: 0.9328467153284672\n",
            "\t\tDataset 1 final accuracy: 0.9144351647659029\n",
            "===>Training dataset: 2\n",
            "\tDataset 2: Accuracy from training 0: 0.9512195121951219\n",
            "\tDataset 2: Accuracy from training 1: 0.9184782608695652\n",
            "\tDataset 2: Accuracy from training 2: 0.9551020408163265\n",
            "\tDataset 2: Accuracy from training 3: 0.9218241042345277\n",
            "\tDataset 2: Accuracy from training 4: 0.9293478260869565\n",
            "\tDataset 2: Accuracy from training 5: 0.9277389277389277\n",
            "\tDataset 2: Accuracy from training 6: 0.9244897959183674\n",
            "\tDataset 2: Accuracy from training 7: 0.9402173913043478\n",
            "\tDataset 2: Accuracy from training 8: 0.933115823817292\n",
            "\tDataset 2: Accuracy from training 9: 0.9406528189910978\n",
            "\t\tDataset 2 final accuracy: 0.9342186501972529\n",
            "===>Training dataset: 3\n",
            "\tDataset 3: Accuracy from training 0: 0.9865067466266867\n",
            "\tDataset 3: Accuracy from training 1: 0.99\n",
            "\tDataset 3: Accuracy from training 2: 0.9909977494373593\n",
            "\tDataset 3: Accuracy from training 3: 0.9904019196160768\n",
            "\tDataset 3: Accuracy from training 4: 0.9835\n",
            "\tDataset 3: Accuracy from training 5: 0.9854264894984998\n",
            "\tDataset 3: Accuracy from training 6: 0.9861215303825956\n",
            "\tDataset 3: Accuracy from training 7: 0.982\n",
            "\tDataset 3: Accuracy from training 8: 0.9855985598559855\n",
            "\tDataset 3: Accuracy from training 9: 0.9819967266775778\n",
            "\t\tDataset 3 final accuracy: 0.9862549722094782\n",
            "===>Training dataset: 4\n",
            "\tDataset 4: Accuracy from training 0: 0.9921951219512194\n",
            "\tDataset 4: Accuracy from training 1: 0.9915474642392718\n",
            "\tDataset 4: Accuracy from training 2: 0.9951219512195122\n",
            "\tDataset 4: Accuracy from training 3: 0.9941474834178697\n",
            "\tDataset 4: Accuracy from training 4: 0.9938211382113821\n",
            "\tDataset 4: Accuracy from training 5: 0.9952619843924192\n",
            "\tDataset 4: Accuracy from training 6: 0.9960975609756098\n",
            "\tDataset 4: Accuracy from training 7: 0.9960979839583785\n",
            "\tDataset 4: Accuracy from training 8: 0.9968780487804878\n",
            "\tDataset 4: Accuracy from training 9: 0.9964526427811281\n",
            "\t\tDataset 4 final accuracy: 0.9947621379927278\n"
          ]
        }
      ]
    }
  ]
}